# Clarification Q&A - README.md Changes

**Purpose**: Clarify changes made to README.md before propagating to constitution, specs, and copilot instructions
**Total Questions**: 100
**Format**: Multiple choice with notes section
**Status**: Draft for review

---

## Section 1: Template Files & Primary Users (Questions 1-10)

### Q4: Should tasks.md be generated by LLM alone or collaboratively with humans?

A) LLM only
B) Human only
C) Human + Agent (RECOMMENDED)
D) Agent only

**Notes**:
Agent will Create, Human will Review and Revise

---

### Q5: What's the relationship between README.md and spec.md? A

A) README.md is for usage guide, spec.md is for product specification (RECOMMENDED)
B) They serve the same purpose
C) README.md replaces spec.md
D) spec.md replaces README.md

**Notes**:

---

### Q8: Why was analyze.md added to the template files?

A) For code analysis
B) For cross-artifact consistency checks before implementation (RECOMMENDED)
C) For performance analysis
D) For security analysis

**Notes**:
I want clear inventory of files that will be in the root directory of specs\001-cryptoutil,
versus all other files being separated into subdirectories, so the root of
a speckit directory only has core speckit files, not all the bloat you created in previous
chat sessions like in specs\001-cryptoutil

---

### Q9: Should implement.md track progress iteratively or only at the end? B

A) Only at the end
B) Iteratively updated and committed during implementation (RECOMMENDED)
C) Never tracked
D) Only at checkpoints

**Notes**:
IMPORTANT to do iterative updates, commits, and pushes, because it triggers pre-commit hooks that
enforce many quality guarantees.

---

### Q16: What should the Implement step track? B, with IMPORTANT CLARIFICATION in NOTES

A) Only completed tasks
B) Append-only timeline in implement/DETAILED.md (RECOMMENDED)
C) Only failed tasks
D) Test results only

**Notes**:
implement/DETAILED.md should have TWO SECTIONS:

1. Checklist of all of the tasks from task.md, in the other that they appear in task.md
2. Append-only timeline in the order that the tasks are implemented.

1 maintains same order as tasks.md, for each cross-reference between implement/DETAILED.md and tasks.md
2 allows is time-ordered, and allows tracking out-of-order implementation of the tasks in section 1,
with references back to the tasks in section 1

---

### Q17: When should tests and coverage be run during Implement? B with IMPORTANT CLARIFICATION IN NOTES

A) Only at the end
B) Before finishing each task (RECOMMENDED)
C) Never
D) Only when requested

**Notes**:
If running tests is needed multiple times during implementing of a task, that is OK.
However, the final test before commit but include coverage.

---

### Q18: What blocks task completion during Implement? C

A) Only test failures
B) Only coverage regression
C) Test failures AND coverage regression (RECOMMENDED)
D) Nothing blocks it

**Notes**:

---

### Q19: Where should the Executive Summary be tracked? A

A) implement/EXECUTIVE.md (RECOMMENDED)
B) implement/DETAILED.md
C) PROGRESS.md
D) PROJECT-STATUS.md

**Notes**:

---

### Q20: What should the Executive Summary include?

A) Only stakeholder overview
B) Stakeholder overview, demonstrability, risk tracking, post mortem (RECOMMENDED)
C) Only technical details
D) Only code metrics

**Notes**:
I ALREADY ANSWERED THIS IN README!!!

---

### Q21: Should Executive Summary notes be appended iteratively or created at the end? B

A) Only at the end
B) Iteratively appended and committed during Implement (RECOMMENDED)
C) Never committed
D) Only in final commit

**Notes**:
Executive Summary can source information from DETAILED.md, so try to note duplicate what is in DETAILED.md.
Executive Summary should append high-level notes or draft content, that will be useful when the Executive Summary
needs to be written much later, after many dozens of different chat sessions. Executive Summary can serve as a
high-level memory that survives different chat sessions, that would otherwise be at risk for loss.

---

### Q22: What does "Customer Demonstrability" include? B and C

A) Only documentation
B) Docker compose up+down, e2e demos, demo videos (RECOMMENDED)
C) Only code samples
D) Only API docs

**Notes**:
Everything must be intuitive to satisfy goal of light documentation.

---

### Q23: What does Risk Tracking cover in Executive Summary? I ALREADY ANSWERED THIS IN README

A) Only known issues
B) Known issues, limitations, missing/incomplete features+tasks (RECOMMENDED)
C) Only security risks
D) Only performance risks

**Notes**:

---

### Q24: What should Post Mortem include? I ALREADY ANSWERED THIS IN README

A) Only what went well
B) What went well, improvements needed, lessons learned, suggestions for docs (RECOMMENDED)
C) Only failures
D) Only metrics

**Notes**:

---

### Q25: Is the Spec Kit workflow linear or can steps be revisited? B and C

A) Strictly linear, never revisit
B) Can iterate and refine after initial completion (RECOMMENDED)
C) Completely flexible, any order
D) Only forward, no backward

**Notes**:

---

## Section 3: Coverage Targets Changes (Questions 26-35)

### Q26: What is the new coverage target for production code? B

A) 90%
B) 95% (RECOMMENDED)
C) 100%
D) 80%

**Notes**:

---

### Q27: What changed for infrastructure (cicd) coverage target? B

A) Increased from 95% to 100%
B) Decreased from 100% to 95% (RECOMMENDED)
C) Stayed at 100%
D) Stayed at 95%

**Notes**:

---

### Q28: What changed for utility code coverage target? B

A) Increased from 95% to 100%
B) Decreased from 100% to 95% (RECOMMENDED)
C) Stayed at 100%
D) Stayed at 90%

**Notes**:

---

### Q29: Are all coverage targets now the same percentage? A

A) Yes, all ≥95% (RECOMMENDED)
B) No, they vary
C) Yes, all 100%
D) Yes, all 90%

**Notes**:

---

### Q31: Why were infrastructure coverage targets lowered from 100% to 95%? B and D and A

A) To reduce workload
B) 100% was unrealistic/impractical (RECOMMENDED)
C) Infrastructure doesn't need high coverage
D) To match production targets

**Notes**:

---

### Q32: Does the ≥95% symbol mean "greater than or equal to 95%"? A

A) Yes (RECOMMENDED)
B) No, it means exactly 95%
C) No, it means greater than 95%
D) It's a typo

**Notes**:

---

### Q33: Are mutation score targets affected by Constitution v2.0 changes? B

A) Yes, increased to 90%
B) No, stayed at ≥80% (RECOMMENDED)
C) Yes, decreased to 70%
D) Mutation testing removed

**Notes**:

---

### Q34: Which coverage targets changed in Constitution v2.0? D

A) Only production
B) Only infrastructure
C) Infrastructure and utility (RECOMMENDED)
D) All targets

**Notes**:

---

### Q35: Should striving for >95% coverage still be encouraged even though 95% is the minimum? B

A) No, 95% is sufficient
B) Yes, higher is better if practical (RECOMMENDED)
C) No, exactly 95% is the goal
D) Coverage doesn't matter

**Notes**:

---

## Section 4: Testing Requirements Updates (Questions 36-50)

### Q36: What changed in the unit tests requirement? B

A) Added ≥95% production, ≥100% infra/util
B) Changed to ≥95% production, removed ≥100% infra/util (RECOMMENDED)
C) Nothing changed
D) Removed coverage requirements

**Notes**:

---

### Q37: What does "happy+sad use case coverage" mean? C

A) Test only success scenarios
B) Test only failure scenarios
C) Test both success and failure scenarios (RECOMMENDED)
D) Test user emotions

**Notes**:

---

### Q38: Should integration tests use mocked databases or real databases? B

A) Always mocked
B) Real database (RECOMMENDED)
C) Either is fine
D) No database needed

**Notes**:
The decision point is, can a dependency be run in a local Docker container?

1. If yes, that a dependency CAN be run as a Docker container, then integration and e2e tests MUST use real container; mock not allowed.
2. If no, that a dependency CAN'T be run as a Docker container, then integration and e2e tests MUST use mocks.

These can be run as real, local Docker containers, which means integration and e2e tests MUST use real containers:

1. PostgreSQL
2. Grafana LGTM
3. Otel Collector Contrib

---

### Q39: What was added to Docker Compose testing requirements? B

A) Only standalone testing
B) Standalone per product, Suite of 2-4 products, docker secrets, no env vars (RECOMMENDED)
C) Only suite testing
D) Performance testing

**Notes**:
My edits for B are what the requirements since the beginning. I am reiterating them, so you double check that they are
correctly propagated throughout all docs.

---

### Q40: Why explicitly mention "no environment variables" for Docker Compose tests? A

A) Environment variables are insecure for secrets (RECOMMENDED)
B) Environment variables don't work
C) Docker doesn't support them
D) It's a best practice only

**Notes**:

---

### Q41: What does "standlone+suite" mean for Docker Compose testing? A

A) Test each product individually AND all products together (RECOMMENDED)
B) Test only standalone
C) Test only suite
D) Test neither

**Notes**:

---

### Q42: Should e2e tests include real telemetry or mocked telemetry? A

A) Real telemetry (RECOMMENDED)
B) Mocked telemetry
C) No telemetry
D) Optional telemetry

**Notes**:
The decision point is, can a dependency be run in a local Docker container?

1. If yes, that a dependency CAN be run as a Docker container, then integration and e2e tests MUST use real container; mock not allowed.
2. If no, that a dependency CAN'T be run as a Docker container, then integration and e2e tests MUST use mocks.

These can be run as real, local Docker containers, which means integration and e2e tests MUST use real containers:

1. PostgreSQL
2. Grafana LGTM
3. Otel Collector Contrib

---

### Q43: What file naming convention is required for property-based tests? B

A) \*\_test.go
B) \*\_property\_test.go (RECOMMENDED)
C) \*\_prop\_test.go
D) \*\_invariant\_test.go

**Notes**:
Reason is differentiate \*\_property\_test.go from \*\_fuzz\_test.go to avoid a repeat mistake where CI fuzz workflow was failing
because it incorrectly tried to include running property tests as part of fuzz CI workflow.

---

### Q44: Are property-based tests mandatory or optional? B

A) Mandatory for all code
B) Mandatory for specific use cases (crypto, invariants) (RECOMMENDED)
C) Optional
D) Deprecated

**Notes**:

---

### Q45: What was clarified about Docker Compose testing requirements? B

A) Only functional testing
B) Full stack with real dependencies (RECOMMENDED)
C) Only unit testing
D) No specific requirements

**Notes**:
Docker Compose must be the superset of production and test services.
Customers should be able to copy it, replace all secrets, remove test service (e.g. cryptoutil-sqlite),
and deployment, and be confident knowing it is secure by default.

---

### Q46: Should test database be in-memory or persistent for Docker Compose tests? B and A

A) Always in-memory
B) Real database (RECOMMENDED)
C) No database
D) Either is fine

**Notes**:
Postgres container must use a Docker volume that survives Postgres or Docker Compose restarts.
SQLite in-memory DB inside services (e.g. kms-cryptoutil-sqlite) must be in-memory only.

---

### Q47: What does "real telemetry" mean in testing context? B

A) Production telemetry backend
B) Actual OTLP/Prometheus/logging infrastructure (RECOMMENDED)
C) No telemetry
D) Mocked telemetry

**Notes**:
Otel Collector Contrib: All services in all 4 products will forward all telemetry to otel-collector-contrib container.
Grafana LGTM: otel-collector-contrib will forward to Grafana LGTM, where LGTM acronym means Loki for Logs, Grafana for GUI, Tempo for Traces, Prometheus for Metrics

---

### Q48: Why emphasize both "standalone" and "suite" Docker Compose testing? A

A) To catch integration issues between products (RECOMMENDED)
B) For redundancy
C) To increase test coverage
D) No specific reason

**Notes**:

1. Every product must support standalone deployment
2. Every product must support optional, configuration-based cooperation as a suite.

Example: KMS can run standalone with simple file realm config, or federate to Identity product, or combine both.

---

### Q49: Should e2e tests include demo scripts?

A) No, separate from tests
B) Yes, demo scripts are part of e2e validation (RECOMMENDED)
C) Optional
D) Only for production

**Notes**:
The e2e tests ARE the demo scripts. Demos must be implemented in Go, not scripting languages.

---

### Q50: What's the relationship between e2e tests and demo videos? None of these

A) Unrelated
B) Demo videos demonstrate e2e test scenarios (RECOMMENDED)
C) Demo videos replace e2e tests
D) Demo videos are deprecated

**Notes**:
Demo videos can be videos or gifs that can be embedded in Markdown files for viewing in GitHub.com projects or Product Website.
They will show how to run each of the high-level customer focused commands, and show the working UIs and/or APIs associated.
For example, a KMS standalone demo video would show `docker compose up -d` and results, open UI, show happy path of UI (e.g KMS login, create ElasticKey, create MaterialKey, encrypt/decrypt with ElasticKey, KMS logout), and then show `docker compose down -v`.

---

## Section 5: Quality Gates Updates (Questions 51-60)

### Q53: Why specify "CI workflows passing" instead of just "CI passing"? A

A) More specific about multiple workflows (RECOMMENDED)
B) Same meaning
C) Typo
D) Workflows is the new term

**Notes**:
You frequently make incorrect assertions that CI is working, when in fact only some workflows passed, and some failed.
CI passing requires all workflows to pass, hence why I clarified CI workflows passing.

---

### Q54: Should DAST run before or after merge? A

A) Before merge (pre-merge gate) (RECOMMENDED)
B) After merge
C) Never
D) Optional

**Notes**:
DAST must run as part of every commit in a PR and after every merge!!!!

---

### Q55: Should SAST run before or after merge? A

A) Before merge (pre-merge gate) (RECOMMENDED)
B) After merge
C) Never
D) Optional

**Notes**:
SAST must run as part of every commit in a PR and after every merge!!!!

---

### Q57: Is formatting enforcement part of pre-commit or pre-push? A

A) Pre-commit (RECOMMENDED)
B) Pre-push
C) Pre-merge
D) Manual only

**Notes**:

---

### Q58: Should format checks auto-fix or just report? A

A) Auto-fix when possible (RECOMMENDED)
B) Only report
C) Block commit without fix
D) Optional

**Notes**:

---

### Q59: Are pre-commit hooks sufficient for code quality? B

A) Yes, pre-commit is enough
B) No, need pre-commit + pre-push + pre-merge + CI workflows passing (RECOMMENDED)
C) No hooks needed
D) Only pre-merge matters

**Notes**:

---

### Q60: What's the purpose of having multiple quality gate stages? B

A) Redundancy
B) Fast feedback at commit, comprehensive validation at push/merge (RECOMMENDED)
C) Slow down development
D) No specific purpose

**Notes**:

---

## Section 6: FIPS 140-3 Requirements (Questions 61-70)

### Q61: What was added to symmetric encryption approved algorithms? B

A) 3DES
B) AES-HS ≥256 bits (RECOMMENDED)
C) DES
D) ChaCha20

**Notes**:
It should say AES-HS ≥256 bits, not AES-HS ≥128 bits.
Minimum AES-HS is 256-bit, double minimum requirement for AES without HS.
AES HMAC-SHA (for AEAD modes) requirements come from the JOSE JWA RFC standard, and cryptoutil KMS already supports AES-HS algorithms.

---

### Q62: What does AES-HS stand for? B

A) AES High Security
B) AES HMAC-SHA (for AEAD modes) (RECOMMENDED)
C) AES Hardware Support
D) AES Hash Standard

**Notes**:

---

### Q63: Why was AES-HS added to the approved algorithms list? None of these

A) New NIST standard
B) Clarify AEAD modes like AES-GCM require HMAC-SHA (RECOMMENDED)
C) Better performance
D) Industry best practice

**Notes**:
AES HMAC-SHA (for AEAD modes) requirements come from the JOSE JWA RFC standard, and cryptoutil KMS already supports AES-HS algorithms.

---

### Q64: Are EdDSA algorithms FIPS 140-3 approved? A

A) Yes (RECOMMENDED)
B) No
C) Only Ed25519
D) Only Ed448

**Notes**:

---

### Q65: What's the minimum RSA key size for FIPS compliance? B

A) 1024 bits
B) 2048 bits (RECOMMENDED)
C) 3072 bits
D) 4096 bits

**Notes**:

---

### Q66: Are NIST P-256, P-384, P-521 curves approved? A

A) Yes (RECOMMENDED)
B) No
C) Only P-256
D) Only P-521

**Notes**:

---

### Q67: Why is bcrypt banned despite being widely used? B

A) Poor performance
B) Not FIPS 140-3 approved (RECOMMENDED)
C) Security vulnerabilities
D) Not supported by Go

**Notes**:

---

### Q68: What should be used instead of bcrypt for password hashing? C

A) MD5
B) SHA-256
C) PBKDF2-HMAC-SHA256/384/512 (RECOMMENDED)
D) Argon2

**Notes**:

---

### Q69: Can FIPS mode be disabled for development/testing? B

A) Yes, for convenience
B) No, ALWAYS enabled, NEVER disabled (RECOMMENDED)
C) Yes, but only in development
D) Yes, with approval

**Notes**:

---

### Q70: Why is the FIPS mode requirement so strict (ALWAYS enabled)? C, B, D

A) Legal compliance requirements (RECOMMENDED)
B) Performance reasons
C) Security best practice only
D) Industry standard

**Notes**:
Enforce highest security standards, even for testing.
Avoid the need for matrix-like testing of non-FIPS vs FIPS mode, which would double the amount of testing.

---

## Section 7: Constitution Integration Changes (Questions 71-85)

### Q71: What was added to Section II about data at rest? B

A) Encryption requirements
B) Confidentiality and integrity (RECOMMENDED)
C) Backup requirements
D) Compression

**Notes**:

---

### Q72: What was added to Section II about data in transit? B

A) TLS requirements
B) Confidentiality and integrity (RECOMMENDED)
C) Protocol requirements
D) Network security

**Notes**:

---

### Q73: Why emphasize both "confidentiality" and "integrity"? B, with CLARIFICATION in Notes

A) Redundancy
B) CIA triad - both are critical security properties (RECOMMENDED)
C) Legal requirement
D) Best practice only

**Notes**:
CIA Extended triad

---

### Q74: What was clarified about shared unseal secrets? B

A) Always unique per instance
B) Shared in microservices sharing a database, for interoperability (RECOMMENDED)
C) Never shared
D) Optional sharing

**Notes**:

---

### Q75: Why do microservices sharing a database need shared unseal secrets? B

A) Simplicity
B) Cryptographic interoperability (RECOMMENDED)
C) Performance
D) Security

**Notes**:
If unseal keys don't match in microservices reading from same database,
they won't be able to encrypt/decrypt the data. All sensitive data is
encrypted by 1 of N microservices that share the database. The other N-1
microservices MUST use the same unseal keys to be able to decrypt that data.

---

### Q76: What new section was added to Constitution integration? B

A) Section III: Workflow Requirements
B) Section IV: Go Production Requirements (RECOMMENDED)
C) Section VII: Docker Requirements
D) Section VIII: Testing Requirements

**Notes**:

---

### Q77: What was clarified about UUID type in databases? B

A) Use native UUID type
B) Use TEXT type (not UUID) - breaks SQLite (RECOMMENDED)
C) Use BLOB type
D) Use VARCHAR type

**Notes**:

---

### Q78: Why must UUID be stored as TEXT instead of UUID type? B

A) Better performance
B) SQLite doesn't support native UUID type (RECOMMENDED)
C) PostgreSQL requirement
D) GORM limitation

**Notes**:

---

### Q79: What should be used for nullable UUIDs instead of pointers? B

A) SQL NULL
B) NullableUUID type (RECOMMENDED)
C) Empty string
D) Zero UUID

**Notes**:
ANALYSIS: The NullableUUID type (internal/identity/domain/nullable_uuid.go) is a custom type that implements
sql.Scanner and driver.Valuer interfaces. It properly handles serialization/deserialization of nullable UUIDs
to/from TEXT columns in SQLite databases. Without this custom type, pointer UUIDs (*googleUuid.UUID) cause
"SQL logic error: row value misused" errors in SQLite with GORM because they don't serialize correctly to TEXT columns.

The type has three key methods:

1. Scan() - Implements sql.Scanner to read NULL or string/[]byte UUIDs from database
2. Value() - Implements driver.Valuer to write NULL or UUID string to database
3. Ptr() - Helper to convert back to *googleUuid.UUID for application code

This pattern is documented in 01-04.database.instructions.md and was discovered during Device Authorization Grant
implementation when pointer UUIDs failed with "row value misused" errors in SQLite.

---

### Q80: Why use NullableUUID instead of *uuid.UUID pointers? B

A) Better performance
B) Prevents "row value misused" errors in SQLite (RECOMMENDED)
C) GORM requirement
D) PostgreSQL compatibility

**Notes**:
ANALYSIS: Pointer UUIDs (*googleUuid.UUID) don't serialize correctly to TEXT columns in SQLite with GORM.
When GORM tries to scan a NULL or TEXT value into a pointer UUID field, it causes "SQL logic error: row value misused"
because the googleUuid.UUID type doesn't implement the sql.Scanner and driver.Valuer interfaces correctly for nullable
cases. The NullableUUID type fixes this by:

1. Explicitly tracking Valid bool flag to distinguish NULL from valid UUID
2. Implementing sql.Scanner to handle NULL, string, and []byte database values
3. Implementing driver.Valuer to return nil for NULL or UUID string for valid values
4. Working correctly with both SQLite (TEXT columns) and PostgreSQL (TEXT or UUID columns)

This is a cross-database compatibility pattern required when using GORM with nullable UUID foreign keys in models.

---

### Q81: What was clarified about JSON fields in GORM models? B

A) Use type:json
B) Use serializer:json (not type:json) (RECOMMENDED)
C) Don't use JSON
D) Use JSONB

**Notes**:

---

### Q82: Why use serializer:json instead of type:json? B

A) Better performance
B) Cross-DB compatibility (SQLite has no native JSON type) (RECOMMENDED)
C) GORM recommendation
D) PostgreSQL requirement

**Notes**:

---

### Q83: What was clarified about localhost in Docker Compose? B

A) Use "localhost"
B) Use 127.0.0.1 (not "localhost") to avoid IPv6 vs IPv4 issues (RECOMMENDED)
C) Use 0.0.0.0
D) Use hostname

**Notes**:

---

### Q84: Why is 127.0.0.1 preferred over "localhost" in Docker? B

A) Better performance
B) Avoids IPv6 ::1 vs IPv4 127.0.0.1 stack split issues (RECOMMENDED)
C) Docker requirement
D) Alpine Linux requirement

**Notes**:

---

### Q85: What was added about CGO dependency in Section IV? B

A) CGO required
B) Static linking with debug symbols; no CGO dependency (RECOMMENDED)
C) CGO optional
D) CGO for production only

**Notes**:
There is one-and-only-one exception for using CGO in the entire project; the ci-race.yml workflow, due to Go toolchian limitation.
All production code, and all test code except ci-race.yml, MUST NEVER USE CGO.

---

## Section 8: Section V Testing Requirements (Questions 86-95)

### Q86: What was clarified about coverage in Section V? B

A) 100% for all
B) 95% for production/infra/utility (RECOMMENDED)
C) 90% for all
D) No specific targets

**Notes**:

---

### Q87: What was added about test case coverage? B

A) Only happy path
B) Cover happy and sad path use cases (RECOMMENDED)
C) Only sad path
D) Only edge cases

**Notes**:

---

### Q88: What was clarified about magic values in tests? B

A) Use hardcoded values
B) Must use random UUIDv7 or magic constants (RECOMMENDED)
C) Use sequential integers
D) Any value is fine

**Notes**:
This was a requirement from the start, but I want to reiterate it so you propagate it correctly.

---

### Q89: What was added about property-based test file naming? B

A) \*\_test.go
B) \*\_property\_test.go (RECOMMENDED)
C) \*\_gopter\_test.go
D) \*\_invariant\_test.go

**Notes**:

---

### Q90: What was clarified about os.Exit() in library/test code? B

A) Use os.Exit() anywhere
B) NEVER os.Exit() in library/test code (RECOMMENDED)
C) Use os.Exit() only in tests
D) Use os.Exit() only in libraries

**Notes**:
This was a requirement from the start, but I want to reiterate it so you propagate it correctly.

---

## Section 9: Section VI Code Quality Excellence (Questions 96-100)

### Q91: What new principle was added to Section VI? B

A) Fix all bugs
B) Test artifacts MUST match production quality standards (RECOMMENDED)
C) Document everything
D) Optimize performance

**Notes**:
This was a requirement from the start, but I want to reiterate it so you propagate it correctly.

---

### Q92: What does "test artifacts" include? B

A) Only test code
B) Code, config, workflows, automation, security, concurrency, etc (RECOMMENDED)
C) Only test files
D) Only test reports

**Notes**:
This was a requirement from the start, but I want to reiterate it so you propagate it correctly.

---

### Q93: Should linting errors in test files be fixed? B

A) No, tests don't need linting
B) Yes, including tests (RECOMMENDED)
C) Optional
D) Only critical errors

**Notes**:
This was a requirement from the start, but I want to reiterate it so you propagate it correctly.

---

### Q94: What was clarified about linting and formatting errors? B

A) Fix most errors
B) Fix ALL linting and formatting errors (no exceptions); including tests (RECOMMENDED)
C) Fix only production code
D) Auto-fix only

**Notes**:
This was a requirement from the start, but I want to reiterate it so you propagate it correctly.

---

### Q95: Are there any exceptions to the "fix ALL linting errors" rule? D

A) Yes, many exceptions
B) Only for documented linter bugs (RECOMMENDED)
C) Tests are excepted
D) Generated code is excepted

**Notes**:
Generated code, vendored code

---

### Q96: What quality aspects must test artifacts match with production? B

A) Only code quality
B) Code, config, workflows, automation, security, concurrency, performance, scalability, robustness (RECOMMENDED)
C) Only security
D) Only performance

**Notes**:
This was a requirement from the start, but I want to reiterate it so you propagate it correctly.

---

### Q97: Why was the test artifact quality principle added? B

A) Nice to have
B) Tests often held to lower standards, causing production issues (RECOMMENDED)
C) Legal requirement
D) Industry trend

**Notes**:
This was a requirement from the start, but I want to reiterate it so you propagate it correctly.
You are frequently prone to making this mistake in past Chat Sessions, and I want to put an end to it permanently.

---

### Q98: Should test code be reviewed with the same rigor as production code? B

A) No, less rigor for tests
B) Yes, same rigor (RECOMMENDED)
C) More rigor for tests
D) No review needed

**Notes**:
This was a requirement from the start, but I want to reiterate it so you propagate it correctly.

---

### Q99: Should security best practices apply to test code? B

A) No, tests are safe
B) Yes, same standards (RECOMMENDED)
C) Only for integration tests
D) Optional

**Notes**:
This was a requirement from the start, but I want to reiterate it so you propagate it correctly.

---

### Q100: Should concurrency best practices apply to test code? B

A) No, tests are single-threaded
B) Yes, especially with t.Parallel() (RECOMMENDED)
C) Only for performance tests
D) Optional

**Notes**:
This was a requirement from the start, but I want to reiterate it so you propagate it correctly.

---

## Summary of Major Changes

### Template Structure Changes

- Added clarify.md, analyze.md, implement.md to template files
- Clarified primary users (Human vs Agent vs Human+Agent)
- Restructured workflow to be more sequential and clear

### Workflow Changes

- Added Clarify step (after Specify, before Plan)
- Added Analyze step (after Tasks, before Implement)
- Added Executive Summary step (iterative during Implement)
- Clarified that Implement tracks progress in implement/DETAILED.md
- Added requirement for tests+coverage before finishing tasks

### Coverage Target Changes

- Unified all coverage targets to ≥95%
- Lowered infrastructure from 100% to 95%
- Lowered utility from 100% to 95%
- Referenced Constitution v2.0 as source of change

### Testing Requirement Changes

- Added "happy+sad use case coverage" to testability
- Added Docker Compose testing with real database, standalone+suite
- Emphasized "no environment variables" for Docker Compose
- Added property-based test file naming convention

### Quality Gate Changes

- Added "format" to pre-commit gate
- Changed "CI passing" to "CI workflows passing"
- Added "dast, sast, quality" to pre-merge gate

### FIPS Changes

- Added AES-HS ≥128 bits to symmetric encryption algorithms
- Clarified EdDSA approval status

### Constitution Integration Changes

- Added "confidentiality and integrity" to data at rest/transit
- Clarified shared unseal secrets for microservices sharing database
- Added Section IV: Go Production Requirements
- Added Section V: Go Testing Requirements renumbering
- Added Section VI: Code Quality Excellence renumbering
- Added details about UUID TEXT type, NullableUUID, JSON serializer
- Added 127.0.0.1 vs localhost clarification
- Added CGO dependency prohibition
- Added test artifact quality principle
- Added specific coverage percentages to Section V
- Added happy+sad path testing requirement
- Added magic values clarification
- Added property test file naming
- Added os.Exit() prohibition in library/test code
- Added linting requirement clarification for tests

---

**Document Status**: Ready for review

**Next Steps**:

1. Review answers and provide clarifications in Notes sections
2. Update constitution with approved changes
3. Update specs/000-cryptoutil-template with approved changes
4. Update specs/001-cryptoutil with approved changes
5. Update copilot instructions with approved changes
