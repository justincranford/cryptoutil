---
description: "Instructions for testing"
applyTo: "**"
---
# Testing Instructions

## CRITICAL: Test Concurrency - NEVER VIOLATE

ALWAYS use test concurrency

- NEVER use `-p=1` for testing
- ALWAYS use concurrent test execution
- ALWAYS use `-shuffle` option for go test

**Why Concurrent Testing is Mandatory**:

1. **Fastest test execution**: Parallel tests = faster feedback loop
2. **Reveals production bugs**: Race conditions, deadlocks, data conflicts exposed
3. **Production validation**: If tests can't run concurrently, production code can't either
4. **Quality assurance**: Concurrent tests = higher confidence in code correctness and robustness

**Test Execution Commands**:

```bash
# CORRECT - Concurrent with shuffle
go test ./... -cover -shuffle=on

# CORRECT - Default concurrent execution
go test ./...

# WRONG - Sequential package execution (hides concurrency bugs!)
go test ./... -p=1  # ❌ NEVER DO THIS

# WRONG - Sequential test execution
go test ./... -parallel=1  # ❌ NEVER DO THIS
```

## CRITICAL: main() Function Pattern for Maximum Coverage - MANDATORY

**ALL main() functions in the ENTIRE project MUST be thin wrappers calling co-located testable functions:**

```go
// CORRECT - Thin main() delegates to testable internalMain()
func main() {
    os.Exit(internalMain(os.Args, os.Stdin, os.Stdout, os.Stderr))
}

// internalMain is testable - accepts injected dependencies
func internalMain(args []string, stdin io.Reader, stdout, stderr io.Writer) int {
    // All logic here - fully testable with mocks
    if len(args) < 2 {
        fmt.Fprintln(stderr, "usage: cmd <arg>")
        return 1
    }
    // ... business logic
    return 0
}

// WRONG - Logic directly in main() blocks testing
func main() {
    if len(os.Args) < 2 {  // ❌ Untestable - hardcoded os.Args
        fmt.Fprintln(os.Stderr, "usage: cmd <arg>")  // ❌ Untestable - hardcoded os.Stderr
        os.Exit(1)  // ❌ Untestable - terminates process
    }
}
```

**Why This Pattern is MANDATORY**:

- **95%+ coverage achievable**: main() 0% is acceptable when internalMain() is 95%+
- **Dependency injection**: Tests inject mocks for args, stdin, stdout, stderr
- **Exit code testing**: Tests verify return codes without terminating test process
- **Happy/sad path testing**: Test all branches (missing args, invalid input, success cases)
- **Example**: adaptive_sim.go stuck at 63% because large main() with os.* dependencies is untestable

**Pattern for ALL Commands**:

- cmd/cicd/cicd.go → internalMain() testable function
- cmd/cryptoutil/main.go → internalMain() testable function  
- cmd/workflow/main.go → internalMain() testable function
- internal/cmd/cicd/*/main.go → internalMain() testable function

**Testing Pattern**:

```go
func TestInternalMain_HappyPath(t *testing.T) {
    args := []string{"cmd", "arg1"}
    stdin := strings.NewReader("")
    stdout := &bytes.Buffer{}
    stderr := &bytes.Buffer{}

    exitCode := internalMain(args, stdin, stdout, stderr)

    require.Equal(t, 0, exitCode)
    require.Contains(t, stdout.String(), "success")
}

func TestInternalMain_MissingArgs(t *testing.T) {
    args := []string{"cmd"}  // Missing required arg
    stderr := &bytes.Buffer{}

    exitCode := internalMain(args, nil, nil, stderr)

    require.Equal(t, 1, exitCode)
    require.Contains(t, stderr.String(), "usage")
}
```

## CRITICAL: Coverage Analysis BEFORE Writing Tests

**MANDATORY WORKFLOW - NEVER SKIP**:

1. **Generate baseline coverage report**: `go test ./pkg -coverprofile=./test-output/coverage_pkg.out`
2. **Analyze uncovered lines**: `go tool cover -html=./test-output/coverage_pkg.out -o ./test-output/coverage_pkg.html`
3. **Identify specific gaps**: Open HTML report, look for RED (uncovered) lines
4. **Target specific branches**: Write tests ONLY for identified uncovered code
5. **Verify improvement**: Re-run coverage, confirm gaps filled

**Why This Matters**:

- Prevents wasted effort writing tests for already-covered code
- Targets exact missing branches, not guesswork
- Measurable progress per test addition
- Avoids trial-and-error test deletion cycles

**Example Gap Analysis**:

```bash
# Step 1: Generate baseline
go test ./internal/jose -coverprofile=./test-output/coverage_jose_baseline.out

# Step 2: Check per-function coverage
go tool cover -func ./test-output/coverage_jose_baseline.out | Where-Object { $_ -match 'jose.*\.go:' -and $_ -match '(\d+\.\d+)%' -and [double]$Matches[1] -lt 90.0 }

# Step 3: Visual analysis of specific files
go tool cover -html=./test-output/coverage_jose_baseline.out -o ./test-output/coverage_jose_baseline.html
# Open HTML, find RED lines in low-coverage functions

# Step 4: Write targeted tests for RED lines only
# Step 5: Re-run and verify improvement
go test ./internal/jose -coverprofile=./test-output/coverage_jose_new.out
```

## CRITICAL: Test Output File Locations

**MANDATORY - NEVER violate**:

- **ALWAYS place test outputs in `./test-output/`** directory (project root)
- **NEVER place test files in source directories** (e.g., `internal/jose/test-coverage*.out`)
- **Use descriptive names**: `coverage_<package>_<variant>.out` format
- **Examples**:
  - ✅ `./test-output/coverage_jose_baseline.out`
  - ✅ `./test-output/coverage_jose_new.out`
  - ✅ `./test-output/coverage_identity_auth.out`
  - ❌ `internal/jose/test-coverage.out` (pollutes source tree)
  - ❌ `coverage.out` (non-descriptive)

## CRITICAL: Table-Driven Test Patterns - MANDATORY

**ALWAYS use table-driven tests** for both happy path AND sad path test cases:

**Happy Path Pattern** (multiple valid inputs):

```go
func TestCreateJWK_AllAlgorithms(t *testing.T) {
    t.Parallel()
    tests := []struct {
        name string
        alg  Algorithm
        // ... test parameters
    }{
        {name: "HMAC_HS256", alg: AlgHS256, ...},
        {name: "HMAC_HS384", alg: AlgHS384, ...},
        {name: "RSA_2048", alg: AlgRS256, ...},
        // ... all variants
    }
    for _, tt := range tests {
        t.Run(tt.name, func(t *testing.T) {
            t.Parallel()
            // test logic
        })
    }
}
```

**Sad Path Pattern** (multiple error conditions):

```go
func TestCreateJWK_ErrorCases(t *testing.T) {
    t.Parallel()
    tests := []struct {
        name        string
        input       Input
        expectedErr string
    }{
        {name: "NilKid", input: Input{Kid: nil}, expectedErr: "kid required"},
        {name: "NilAlg", input: Input{Alg: nil}, expectedErr: "alg required"},
        {name: "NilKey", input: Input{Key: nil}, expectedErr: "key required"},
        // ... all error cases
    }
    for _, tt := range tests {
        t.Run(tt.name, func(t *testing.T) {
            t.Parallel()
            result, err := CreateJWK(tt.input)
            require.Error(t, err)
            require.Nil(t, result)
            require.Contains(t, err.Error(), tt.expectedErr)
        })
    }
}
```

**NEVER create individual test functions for each variant**:

```go
// ❌ WRONG - violates table-driven mandate
func TestCreateJWK_HMAC_HS256(t *testing.T) { ... }
func TestCreateJWK_HMAC_HS384(t *testing.T) { ... }
func TestCreateJWK_HMAC_HS512(t *testing.T) { ... }
func TestCreateJWK_NilKid(t *testing.T) { ... }
func TestCreateJWK_NilAlg(t *testing.T) { ... }
func TestCreateJWK_NilKey(t *testing.T) { ... }
```

**When to use individual functions**:

- Complex setup requiring helper functions
- Fundamentally different test logic (not just parameter variations)
- Integration tests with distinct scenarios

## Core Rules

- Target coverage: 95%+ production, 100%+ infrastructure (cicd), 100% utility code
- ALWAYS use table-driven tests with `t.Parallel()` - clear, compact, orthogonal data, reveals real concurrency bugs
- ALWAYS use testify `require` for assertions for fast fail
- NEVER os.Exit() in test code - return errors; only main() calls os.Exit()
- ALWAYS check and close response bodies: `defer require.NoError(t, resp.Body.Close())`
- ALWAYS check errors from helper functions
- Mutation testing: ≥80% gremlins score per package (mandatory)
- NEVER create temporary test files requiring deletion - create permanent tests only
- NEVER hardcode test values - use runtime-generated UUIDv7 or magic package constants

**Test Data Isolation Requirements**:

- ALWAYS use unique values: UUIDv7 for all test data (thread-safe, process-safe)
- ALWAYS use dynamic ports: port 0 pattern for all test servers for all products
- ALWAYS use TestMain for dependencies: Start once per package (PostgreSQL containers, service dependencies)
- ALWAYS use real dependencies: Test containers (PostgreSQL, Otel Collector Contrib, Grafana LGTM)
- ONLY use mocks only for: Hard-to-reach corner cases, external dependencies that can't be run as local containers (Cloud-only services)

**Test Values**:

**Option A**: Generate once, reuse: `id := googleUuid.NewV7()` then use `id` in test cases
**Option B**: Magic values from `internal/identity/magic` package
**NEVER**: Inline hardcoded UUIDs, strings, or calling `NewV7()` twice expecting same result

## Test File Organization

| Type | Suffix | Example |
|------|--------|---------|
| Unit | `_test.go` | `calc_test.go` |
| Bench | `_bench_test.go` | `calc_bench_test.go` |
| Fuzz | `_fuzz_test.go` | `calc_fuzz_test.go` |
| Property | `_property_test.go` | `calc_property_test.go` |
| Integration | `_integration_test.go` | `api_integration_test.go` |

## CRITICAL: Race Condition Prevention

- NEVER write to parent scope variables in parallel sub-tests
- NEVER use t.Parallel() with global state manipulation (os.Stdout, env vars)
- ALWAYS use inline assertions: `require.NoError(t, resp.Body.Close())`
- ALWAYS create fresh test data per test case (new sessions, UUIDs)
- ALWAYS protect shared maps/slices with sync.Mutex or sync.Map
- Detection: `go test -race -count=2` (requires CGO_ENABLED=1)
- NEVER compare database timestamps against `time.Now()` in tests with concurrent execution, async operations, or race condition testing

## Mutation Testing (Mandatory)

- Use [gremlins](https://github.com/go-gremlins/gremlins) for mutation testing
- Target: ≥80% mutation score per package
- Run: `gremlins unleash --tags=!integration` (excludes integration tests)
- Focus on business logic, parsers, validators, crypto operations
- Create baseline report and track improvements in docs/GREMLINS-TRACKING.md
- Always use Package-level parallelization: Run gremlins on packages concurrently using workflow matrix

**Recommended Configuration** (`.gremlins.yaml`):

```yaml
threshold:
  efficacy: 80  # Target: ≥80% test efficacy
  mutant-coverage: 70  # Target: ≥70% mutant coverage

workers: 4  # Parallel mutant execution
test-cpu: 2  # CPU per test run
timeout-coefficient: 2  # Timeout multiplier
```

## Benchmarking (Mandatory for Crypto)

- **ALWAYS create benchmarks for cryptographic operations** (happy and sad paths)
- File suffix: `_bench_test.go`
- Run: `go test -bench=. -benchmem ./pkg/crypto`
- Track performance regressions in CI/CD
- Examples: key generation, encryption/decryption, signing/verification, hashing

```go
func BenchmarkAESEncrypt(b *testing.B) {
    key := make([]byte, 32)
    plaintext := make([]byte, 1024)
    b.ResetTimer()
    for i := 0; i < b.N; i++ {
        _, _ = encrypt(key, plaintext)
    }
}
```

## Fuzz Testing (Mandatory)

- **ALWAYS create fuzz tests for parsers, validators, and input handlers**
- **CRITICAL: Fuzz tests MUST ONLY contain fuzz functions (Fuzz*), NOT unit/property/integration tests**
- Use `//go:build !fuzz` tag to exclude property tests from fuzz test runs
- Fuzz test names MUST be unique, NOT substrings of others (e.g., `FuzzHKDFAllVariants` not `FuzzHKDF`)
- ALWAYS run from project root: `go test -fuzz=FuzzXXX -fuzztime=15s ./path`
- Minimum fuzz time: 15 seconds per test
- Use unquoted names, PowerShell `;` for chaining

**File Organization**:

- `*_fuzz_test.go`: ONLY fuzz functions (FuzzX), no unit tests
- `*_test.go`: Unit, integration, table-driven tests

## Property-Based Testing (Mandatory)

- Use [gopter](https://github.com/leanovate/gopter) for property-based testing
- Focus on invariants and mathematical properties
- File suffix: `_property_test.go`
- Examples: encryption(decryption(x)) == x, sign(verify(x)) == x

```go
func TestEncryptionRoundTrip(t *testing.T) {
    properties := gopter.NewProperties(nil)
    properties.Property("encrypt then decrypt returns original", prop.ForAll(
        func(plaintext []byte) bool {
            ciphertext, _ := Encrypt(key, plaintext)
            result, _ := Decrypt(key, ciphertext)
            return bytes.Equal(plaintext, result)
        },
        gen.SliceOf(gen.UInt8()),
    ))
    properties.TestingRun(t)
}
```

## CRITICAL: Test File Size Limits - MANDATORY

**File Size Targets (STRICTLY ENFORCED)**:

- **Soft limit**: 300 lines (ideal target)
- **Medium limit**: 400 lines (acceptable with justification)
- **Hard limit**: 500 lines (NEVER EXCEED - refactor required)

**Why Size Limits Matter**:

- Faster LLM processing and token usage
- Easier human review and maintenance
- Better test organization and discoverability
- Forces logical test grouping

**When file exceeds 400 lines**:

1. Split by functionality: `jwk_util_test.go` → `jwk_util_create_test.go` + `jwk_util_validate_test.go`
2. Split by algorithm type: `crypto_test.go` → `crypto_rsa_test.go` + `crypto_ecdsa_test.go`
3. Extract test helpers to `*_test_util.go` files
4. Move integration tests to `*_integration_test.go`

**Example Refactoring**:

```
❌ jwk_util_test.go (1371 lines) - VIOLATES HARD LIMIT

✅ jwk_util_create_test.go (280 lines) - CreateJWK* functions
✅ jwk_util_validate_test.go (250 lines) - validateOrGenerate* functions
✅ jwk_util_extract_test.go (180 lines) - Extract* functions
✅ jwk_util_is_test.go (150 lines) - Is* functions
✅ jwk_util_test_util.go (120 lines) - Shared test helpers
```

## Test Execution

Place test results and coverage files in `./test-output/`: `go test ./pkg -coverprofile=./test-output/coverage_pkg.out`

| Directory | Purpose | Examples |
|-----------|---------|----------|
| `./test-output/` | Test result outputs | Coverage files, test logs, Report outputs | Autoapprove logs, CI reports |
| `./testdata/` or `pkg/testdata/` | Test input fixtures | YAML configs, test files |

## Dynamic Port Allocation Pattern (Mandatory for Server Tests)

**ALWAYS use port 0 and extract actual assigned port** for server tests to enable concurrent test execution:

**Why**: Hard-coded ports cause port conflicts when tests run in parallel. Dynamic allocation ensures each test gets a unique port.

## cicd Utility Testing

- Commands organized as `internal/cmd/cicd/<snake_case>/` subdirectories
- EVERY command MUST exclude its own subdirectory (self-exclusion pattern)
- Define exclusion in `internal/common/magic/magic_cicd.go`
- Add self-exclusion test to verify pattern works
- Target 95%+ coverage per command subdirectory

## Common Testing Anti-Patterns and Lessons Learned

**NEVER DO THESE** - Lessons from actual session mistakes:

### Anti-Pattern 1: Writing Tests Without Baseline Coverage Analysis

**Mistake**: Adding 60+ tests without analyzing baseline coverage HTML first
**Result**: 0% coverage improvement - tests duplicated already-covered paths
**Correct Pattern**: ALWAYS generate baseline, analyze HTML for RED lines, target specific gaps

### Anti-Pattern 2: Individual Test Functions Instead of Table-Driven

**Mistake**: Creating TestFunc_Variant1, TestFunc_Variant2, TestFunc_Variant3 as separate functions
**Result**: 1371-line test file (2.7x hard limit), maintenance nightmare, slower LLM processing
**Correct Pattern**: Use table-driven tests with variants as rows (see examples above)

### Anti-Pattern 3: Test Outputs in Source Directories

**Mistake**: Placing coverage files in internal/jose/test-coverage.out
**Result**: Source tree pollution, confuses version control, clutters package directories
**Correct Pattern**: ALWAYS use ./test-output/ directory for all test artifacts

### Anti-Pattern 4: Exceeding File Size Limits

**Mistake**: Allowing test files to grow to 1371 lines without refactoring
**Result**: Slower LLM processing, harder maintenance, poor discoverability
**Correct Pattern**: Split at 500 lines using functional grouping (see examples above)

### Anti-Pattern 5: Trial-and-Error Test Writing

**Mistake**: Writing tests, checking coverage, writing more tests, repeat cycle
**Result**: Wasted effort, unclear progress, no measurable improvement strategy
**Correct Pattern**: Baseline → HTML analysis → targeted test → verify improvement cycle

**Key Insight**: Coverage ≠ test count. Many tests can add 0% coverage if exercising already-covered code paths. HTML baseline analysis eliminates guesswork and waste.
