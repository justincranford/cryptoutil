---
description: "Instructions for testing"
applyTo: "**"
---
# Testing Instructions

- Run `go test ./... -cover` for automated tests
- Use testify require methods for assertions; ALWAYS use clear, concise assertions with good failure messages
- Use manual tests via Swagger UI (see README)
- Ensure coverage for all key types and pool configs
- Update/fix tests, run formatters (`golangci-lint run --fix ./...`), and run linters (`golangci-lint run ./...`), before committing
- Script testing: always test scripts after add/update tests, verify help/params, test functional/error/cross-platform paths, document results (see README for details)
- Test directories may contain non-Go performance testing tools (e.g., Java Gatling in `/test/load/`)
- Use constants for repeated test values if it improves clarity; prefer meaningful test data
- When updating dependencies: run `go test ./... -cover` first to confirm code and tests work, and get coverage baseline, before attempting updates; only after tests pass, update dependencies in small batches, and repeat `go test ./... -cover` to iterate on fixing any issues caused by the update, and fixing any regressions in coverage

## Test Structure Best Practices

- **ALWAYS use parameterized table-driven tests** - they have clearer intent, easier to understand, more compact, less duplication, easy to add more cases, better failure reporting, and represent Go testing best practices
- **ALWAYS cover happy and sad path scenarios** in all test classes; include edge cases, invalid inputs, error conditions
- **ALWAYS run tests after writing/modifying them** to verify correctness and catch bug issues early, and catch coverage regressions early
- **ALWAYS analyze existing and new tests** for optimization opportunities: faster execution, better coverage, higher quality assertions
- **Target test coverage**: Maintain 80% or higher coverage for production code; 85% or higher for infrastructure code (e.g., cicd utilities); 95% or higher for utility code

## Test Execution Best Practices

### Coverage File Management
- **ALWAYS place temporary coverage files in `/test-output/` directory** to avoid cluttering project root
- **Pattern**: `go test ./internal -coverprofile=test-output/coverage_pkg.out`
- **Rationale**: Single `.gitignore` entry (`/test-output/`) instead of multiple individual patterns
- **Cleanup**: Manual deletion of `/test-output/` directory contents as needed

### Test Execution Performance
- **CRITICAL: Be cautious with long-running test suites** (database integration tests, ORM tests, internal/client tests, internal/server tests)
- **Monitor test duration**: Some packages (sqlrepository, orm, client) can take 5-10+ minutes
- **Use targeted test runs**: Run specific test functions when iterating
- **Example**: `go test ./internal/server/repository/orm -run=TestSpecificFunction` instead of full suite
- **Rationale**: Frequent full test suite runs massively slow down development progress

### Test File Lifecycle
- **ALWAYS create permanent test files** that can be used going forward
- **NEVER create temporary test files** that require deletion (deletion needs manual user approval)
- **Pattern**: Add tests to existing test files or create new permanent test files
- **Rationale**: Persistent tests serve as regression prevention and documentation

## Dependency Management Best Practices

- **Automated Checks**: Use `go-update-direct-dependencies` in pre-commit hooks and CI/CD workflows for efficient, focused dependency updates
- **Avoid**: `go-update-all-dependencies` in automated contexts as it can cause unnecessary updates and potential compatibility issues
- **Manual Updates**: Use `go-update-all-dependencies` only for intentional comprehensive dependency refreshes during major version updates or maintenance windows

## Test File Organization

Follow Go testing file naming conventions for proper organization:

| Test Type | File Suffix | Purpose | Example |
|-----------|-------------|---------|---------|
| Unit Tests | `_test.go` | Blackbox/whitebox testing of functions | `calculator_test.go` |
| Benchmarks | `_bench_test.go` | Performance testing | `calculator_bench_test.go` |
| Fuzz Tests | `_fuzz_test.go` | Property-based testing | `calculator_fuzz_test.go` |
| Integration | `_integration_test.go` | Component interaction testing | `api_integration_test.go` |
| E2E | `*_test.go` with `//go:build e2e` | Full system end-to-end testing | `e2e_test.go` |

**File Separation Rules:**
- Keep unit tests, benchmarks, and fuzz tests in separate files
- Use descriptive file names that indicate the test focus
- Group related tests by functionality within each file type
- Keep file sizes manageable for readability and maintainability

## Fuzz Testing Guidelines

### CRITICAL: Unique Fuzz Test Naming Convention
- **ALL Fuzz* test function names MUST be unique and MUST NOT be substrings of any other fuzz test names**
- This ensures cross-platform compatibility without quotes or regex in `-fuzz` parameters
- **Example Problem**: `FuzzHKDF` conflicts with `FuzzHKDFwithSHA256` (substring match)
- **Solution**: Use `FuzzHKDFAllVariants` instead of `FuzzHKDF`
- **Why**: Go's `-fuzz` parameter does partial matching; unique names eliminate ambiguity
- **Cross-Platform**: Unquoted parameters work identically on Windows and Linux: `go test -fuzz=FuzzXXX`

### Common Mistakes to Avoid
- **NEVER do this**: Running fuzz tests from subdirectories (breaks Go module detection, causes "go.mod file not found" errors)
- **ALWAYS run Go fuzz tests from project root** - never use `cd` commands before `go test -fuzz` (causes module detection failures)
- **NEVER do this**: Using quotes or regex when test names are unique: `-fuzz="^FuzzXXX$"` (causes cross-platform issues)
- **NEVER do this**: Creating fuzz test names that are substrings of other fuzz test names

### Correct Fuzz Test Execution
- **ALWAYS do this**: Run from project root: `go test -fuzz=FuzzSpecificTest -fuzztime=5s ./internal/common/crypto/keygen`
- **ALWAYS do this**: Specify full package paths: `./internal/common/crypto/digests`
- **ALWAYS do this**: Use PowerShell `;` for chaining: `go test -fuzz=FuzzXXX -fuzztime=5s ./path; echo "Done"`
- **ALWAYS do this**: Use unquoted, unique test names: `-fuzz=FuzzGenerateRSAKeyPair` (no quotes needed)

### Fuzz Test Patterns
- **Specific fuzz test**: `go test -fuzz=FuzzXXX -fuzztime=15s ./<package>` (most common, no quotes)
- **All fuzz tests in package**: `go test -fuzz=. -fuzztime=15s ./<package>` (only if package has 1 fuzz test)
- **Quick verification**: Use `-fuzztime=15s` for fast feedback during development
- **Cross-platform compatibility**: Avoid quotes and regex; ensure unique test names instead

## Test Concurrency and Robustness

### CRITICAL: Parallel Testing Philosophy
- **ALWAYS use `t.Parallel()` in tests whenever possible** - this is a FEATURE, not a problem
- **Parallel tests reveal real concurrency issues** in production code that sequential tests would hide
- **Test isolation via unique data** - use UUIDv7 suffixes to prevent data collisions between concurrent tests
- **Failing parallel tests = production bugs** - if tests fail when run concurrently, the production code has concurrency issues that MUST be fixed

### Why Parallel Testing Matters
- **Validates concurrent safety**: Tests running in parallel expose race conditions, deadlocks, and data isolation bugs
- **Mirrors production reality**: Production code runs concurrently; tests should too
- **Forces robust design**: Code that works correctly with parallel tests is inherently more robust
- **Catches subtle bugs**: Sequential tests can pass while hiding critical concurrency flaws

### Parallel Testing Best Practices
- **Use `t.Parallel()` for all tests** unless there's a specific reason (shared external resource, sequential ordering requirement)
- **Design for isolation**: Each test should create unique orthogonal test data (use UUIDv7 for uniqueness)
- **UUIDv7 over counters/timestamps**: Provides time-ordered uniqueness without collision risk
- **Fix code, not tests**: If parallel tests fail, fix the production code's concurrency issues
- **Test data patterns**: Unique database names, unique entity IDs, unique file paths per test

### Parameterized Tests with Parallelism
- **Combine table-driven tests with `t.Parallel()`** for maximum coverage and concurrency validation
- **Each subtest runs in parallel**: Use `t.Run()` + `t.Parallel()` for isolated, concurrent subtests
- **Example pattern**:
```go
func TestMyFunction(t *testing.T) {
    t.Parallel() // Parent test runs in parallel

    tests := []struct {
        name string
        // ... test cases
    }{
        // ... test data with unique identifiers
    }

    for _, tc := range tests {
        tc := tc // Capture range variable
        t.Run(tc.name, func(t *testing.T) {
            t.Parallel() // Each subtest runs in parallel
            // ... test implementation using tc.uniqueID (UUIDv7)
        })
    }
}
```

### Anti-Patterns to Avoid
- **NEVER remove `t.Parallel()` to fix failing tests** - this hides real bugs
- **NEVER use shared state between parallel tests** - each test must be isolated
- **NEVER rely on test execution order** - parallel tests run in arbitrary order
- **NEVER use sequential counters for uniqueness** - use UUIDv7 instead

## Copilot Testing Guidelines

When testing linting of code samples or validating regex patterns during chat sessions:

- **Create permanent tests in `internal/cmd/cicd/*_test.go`** new or existing test files instead of one-off temporary test files
- This serves as linting regression tests that persist across chat sessions, as well as after "Summarized conversation history"
- Examples: regex validation tests, linting pattern tests, code transformation tests
- All tests in `internal/cmd/cicd/*_test.go` test files execute automatically during Go test runs
- Use descriptive test function names that indicate the validation purpose (e.g., `TestEnforceTestPatterns_RegexValidation`)

### cicd Utility Testing Patterns

When updating the cicd utility:

- **Test pattern**: Tests must write generated code to temporary file (use `t.TempDir()`), run lint/check function against it, assert results programmatically
- **Avoid interactive prompts**: This prevents unwanted prompts during Copilot-assisted sessions
- **Avoid ephemeral shell commands**: Don't use piping PowerShell Get-Content replace commands - prefer programmatic edits via tests or scripted tools
- **Rationale**: Ephemeral shell patterns trigger unwanted prompts and premium LLM watch requests
